--In Local mode with input file on Hadoop instance:
a = LOAD 'file:///home/hadoop/googlebooks-eng-us-all-2gram-20090715-50-subset.csv'  AS (ngram:chararray, year:int, match_count:int, page_count:int, volume_count:int);

--In MapReduce mode with input file on HDFS:
a = LOAD '/user/mfeng45/googlebooks-eng-us-all-2gram-20090715-50-subset.csv'  AS (ngram:chararray, year:int, match_count:int, page_count:int, volume_count:int);

b = FILTER a BY year == 2003;
-- c = GROUP b by match_count;
d = ORDER b BY match_count DESC;

store d into '/user/mfeng45/super_unique10';